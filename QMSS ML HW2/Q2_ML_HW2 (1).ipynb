{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EuN_q-FXZTAo"
   },
   "source": [
    "# Mahcine Learning HW #2 Question 2\n",
    "\n",
    "## Zehui Wang ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JzWSKV0ZgYo"
   },
   "source": [
    "## Question 2. Classification on red and white wine characteristics:\n",
    "\n",
    "First, import the red and the white wine csv files into separate pandas dataframes from the following website:\n",
    "\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/\n",
    "\n",
    "(Note: you need to adjust the argument for read_csv() from sep=',' to sep=';')\n",
    "\n",
    "Add a new column to each data frame called \"winetype\".  For the white wine dataset label the values in this column with a 0, indicating white wine.  For the red wine dataset, label values with a 1, indicating red wine.  Combine both datasets into a single dataframe.\n",
    "\n",
    "The target data (i.e. the dependent variable) is \"winetype\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "BIvy7l9PNBBa",
    "outputId": "e161efdc-d58b-4db9-979b-b778bbb0551b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>winetype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.075</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.9</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.069</td>\n",
       "      <td>15.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.9964</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.3</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.065</td>\n",
       "      <td>15.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.9946</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.47</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.073</td>\n",
       "      <td>9.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.57</td>\n",
       "      <td>9.5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.36</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.071</td>\n",
       "      <td>17.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.80</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "5            7.4              0.66         0.00             1.8      0.075   \n",
       "6            7.9              0.60         0.06             1.6      0.069   \n",
       "7            7.3              0.65         0.00             1.2      0.065   \n",
       "8            7.8              0.58         0.02             2.0      0.073   \n",
       "9            7.5              0.50         0.36             6.1      0.071   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "5                 13.0                  40.0   0.9978  3.51       0.56   \n",
       "6                 15.0                  59.0   0.9964  3.30       0.46   \n",
       "7                 15.0                  21.0   0.9946  3.39       0.47   \n",
       "8                  9.0                  18.0   0.9968  3.36       0.57   \n",
       "9                 17.0                 102.0   0.9978  3.35       0.80   \n",
       "\n",
       "   alcohol  quality  winetype  \n",
       "0      9.4        5         1  \n",
       "1      9.8        5         1  \n",
       "2      9.8        5         1  \n",
       "3      9.8        6         1  \n",
       "4      9.4        5         1  \n",
       "5      9.4        5         1  \n",
       "6      9.4        5         1  \n",
       "7     10.0        7         1  \n",
       "8      9.5        7         1  \n",
       "9     10.5        5         1  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "dfred = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\", sep=';')\n",
    "dfred['winetype'] = 1\n",
    "dfred.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "vUaq-0sAxK2E",
    "outputId": "0c2eff2b-0d05-43c3-ab79-0e96320538ad"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>winetype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.16</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.045</td>\n",
       "      <td>30.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0.9949</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.47</td>\n",
       "      <td>9.6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.044</td>\n",
       "      <td>28.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>3.22</td>\n",
       "      <td>0.45</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "5            8.1              0.28         0.40             6.9      0.050   \n",
       "6            6.2              0.32         0.16             7.0      0.045   \n",
       "7            7.0              0.27         0.36            20.7      0.045   \n",
       "8            6.3              0.30         0.34             1.6      0.049   \n",
       "9            8.1              0.22         0.43             1.5      0.044   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "5                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "6                 30.0                 136.0   0.9949  3.18       0.47   \n",
       "7                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "8                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "9                 28.0                 129.0   0.9938  3.22       0.45   \n",
       "\n",
       "   alcohol  quality  winetype  \n",
       "0      8.8        6         0  \n",
       "1      9.5        6         0  \n",
       "2     10.1        6         0  \n",
       "3      9.9        6         0  \n",
       "4      9.9        6         0  \n",
       "5     10.1        6         0  \n",
       "6      9.6        6         0  \n",
       "7      8.8        6         0  \n",
       "8      9.5        6         0  \n",
       "9     11.0        6         0  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfwhite = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\", sep=';')\n",
    "dfwhite['winetype'] = 0\n",
    "dfwhite.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "Ti47WFh63Mqg",
    "outputId": "76d93c1d-4ee6-48fb-a41c-baf68baf6130"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>winetype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.075</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.9</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.069</td>\n",
       "      <td>15.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.9964</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.3</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.065</td>\n",
       "      <td>15.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.9946</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.47</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.073</td>\n",
       "      <td>9.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.57</td>\n",
       "      <td>9.5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.36</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.071</td>\n",
       "      <td>17.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.80</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "5            7.4              0.66         0.00             1.8      0.075   \n",
       "6            7.9              0.60         0.06             1.6      0.069   \n",
       "7            7.3              0.65         0.00             1.2      0.065   \n",
       "8            7.8              0.58         0.02             2.0      0.073   \n",
       "9            7.5              0.50         0.36             6.1      0.071   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "5                 13.0                  40.0   0.9978  3.51       0.56   \n",
       "6                 15.0                  59.0   0.9964  3.30       0.46   \n",
       "7                 15.0                  21.0   0.9946  3.39       0.47   \n",
       "8                  9.0                  18.0   0.9968  3.36       0.57   \n",
       "9                 17.0                 102.0   0.9978  3.35       0.80   \n",
       "\n",
       "   alcohol  quality  winetype  \n",
       "0      9.4        5         1  \n",
       "1      9.8        5         1  \n",
       "2      9.8        5         1  \n",
       "3      9.8        6         1  \n",
       "4      9.4        5         1  \n",
       "5      9.4        5         1  \n",
       "6      9.4        5         1  \n",
       "7     10.0        7         1  \n",
       "8      9.5        7         1  \n",
       "9     10.5        5         1  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Joint the two lists\n",
    "\n",
    "df = pd.concat([dfred, dfwhite])\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "JUU2ZcKr5U6U"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>winetype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4888</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.052</td>\n",
       "      <td>38.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>0.99330</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0.54</td>\n",
       "      <td>9.2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4889</th>\n",
       "      <td>4.9</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.27</td>\n",
       "      <td>11.75</td>\n",
       "      <td>0.030</td>\n",
       "      <td>34.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.99540</td>\n",
       "      <td>3.07</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4890</th>\n",
       "      <td>6.1</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.29</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.036</td>\n",
       "      <td>25.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.98938</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.44</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4891</th>\n",
       "      <td>5.7</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.038</td>\n",
       "      <td>38.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>0.99074</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.46</td>\n",
       "      <td>10.6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4892</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.032</td>\n",
       "      <td>29.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.99298</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.54</td>\n",
       "      <td>9.7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "4888            6.8             0.220         0.36            1.20      0.052   \n",
       "4889            4.9             0.235         0.27           11.75      0.030   \n",
       "4890            6.1             0.340         0.29            2.20      0.036   \n",
       "4891            5.7             0.210         0.32            0.90      0.038   \n",
       "4892            6.5             0.230         0.38            1.30      0.032   \n",
       "4893            6.2             0.210         0.29            1.60      0.039   \n",
       "4894            6.6             0.320         0.36            8.00      0.047   \n",
       "4895            6.5             0.240         0.19            1.20      0.041   \n",
       "4896            5.5             0.290         0.30            1.10      0.022   \n",
       "4897            6.0             0.210         0.38            0.80      0.020   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "4888                 38.0                 127.0  0.99330  3.04       0.54   \n",
       "4889                 34.0                 118.0  0.99540  3.07       0.50   \n",
       "4890                 25.0                 100.0  0.98938  3.06       0.44   \n",
       "4891                 38.0                 121.0  0.99074  3.24       0.46   \n",
       "4892                 29.0                 112.0  0.99298  3.29       0.54   \n",
       "4893                 24.0                  92.0  0.99114  3.27       0.50   \n",
       "4894                 57.0                 168.0  0.99490  3.15       0.46   \n",
       "4895                 30.0                 111.0  0.99254  2.99       0.46   \n",
       "4896                 20.0                 110.0  0.98869  3.34       0.38   \n",
       "4897                 22.0                  98.0  0.98941  3.26       0.32   \n",
       "\n",
       "      alcohol  quality  winetype  \n",
       "4888      9.2        5         0  \n",
       "4889      9.4        6         0  \n",
       "4890     11.8        6         0  \n",
       "4891     10.6        6         0  \n",
       "4892      9.7        5         0  \n",
       "4893     11.2        6         0  \n",
       "4894      9.6        5         0  \n",
       "4895      9.4        6         0  \n",
       "4896     12.8        7         0  \n",
       "4897     11.8        6         0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjmXrV_MMF8x"
   },
   "source": [
    "## 2.1 Visualize the univariate distribution of the target feature and each of the three explanatory variables that you think are likely to have a relationship with the target feature.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "X9nXpAnJ_Lbc",
    "outputId": "d0dc6ac0-9499-4a46-fcbe-6d4750d353e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'count')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['winetype'])\n",
    "plt.xlabel('winetype')\n",
    "plt.ylabel('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "dqUzSDalXaxU",
    "outputId": "e5e0718b-3674-42e5-9868-6ae50ec669ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'count')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['fixed acidity'].hist()\n",
    "plt.xlabel('fixed acidity')\n",
    "plt.ylabel('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "PacZmYHPZDhN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'count')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['total sulfur dioxide'].hist()\n",
    "plt.xlabel('total sulfur dioxide')\n",
    "plt.ylabel('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "fhTlF6aLaJUH",
    "outputId": "6bcbcac1-e783-4546-a943-c33f69184fcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'count')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['chlorides'].hist()\n",
    "plt.xlabel('chlorides')\n",
    "plt.ylabel('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMbuekoynqc2"
   },
   "source": [
    "## 2.2 Split data into training and test set. \n",
    "\n",
    "## Build models that evaluate the relationship between all available X variables in the dataset and the target variable. Evaluate Logistic Regression, Penalized Logistic Regression, and KNN for classification using cross-validation. How different are the results? \n",
    "\n",
    "## How does scaling the data with StandardScaler influence the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "id": "1e_HO8xmnrc4",
    "outputId": "15148c87-921f-4eeb-ac59-20c3dafcd03a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df['winetype']\n",
    "X = df.loc[:, df.columns != 'winetype'] #Choose all the raws and all the columns except \"winetype\"\n",
    "\n",
    "\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "2PdHCFWQC27l",
    "outputId": "36cb0237-f555-4ce5-e938-6c311f89bf8c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: winetype, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "SKZrSIafDG-1",
    "outputId": "119688e5-29f8-4d61-b5ca-a8147b675b5b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4893    0\n",
       "4894    0\n",
       "4895    0\n",
       "4896    0\n",
       "4897    0\n",
       "Name: winetype, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "id": "fzvCOf-Fol8T",
    "outputId": "a6a80b96-d875-4d2d-a3b4-05ebaa8eb712"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>7.3</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.36</td>\n",
       "      <td>12.7</td>\n",
       "      <td>0.040</td>\n",
       "      <td>38.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.79</td>\n",
       "      <td>9.60</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3263</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.31</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.030</td>\n",
       "      <td>36.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99026</td>\n",
       "      <td>3.22</td>\n",
       "      <td>0.62</td>\n",
       "      <td>12.60</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3753</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.042</td>\n",
       "      <td>24.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>0.99215</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.46</td>\n",
       "      <td>10.70</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3009</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.50</td>\n",
       "      <td>18.1</td>\n",
       "      <td>0.054</td>\n",
       "      <td>50.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>0.99941</td>\n",
       "      <td>2.94</td>\n",
       "      <td>0.64</td>\n",
       "      <td>8.80</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4493</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.25</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.043</td>\n",
       "      <td>30.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>0.99720</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.55</td>\n",
       "      <td>10.55</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "411             7.3              0.28         0.36            12.7      0.040   \n",
       "3263            6.5              0.26         0.31             3.6      0.030   \n",
       "3753            6.6              0.27         0.33             1.4      0.042   \n",
       "3009            6.5              0.20         0.50            18.1      0.054   \n",
       "4493            6.8              0.31         0.25            10.5      0.043   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "411                  38.0                 140.0  0.99800  3.30       0.79   \n",
       "3263                 36.0                  92.0  0.99026  3.22       0.62   \n",
       "3753                 24.0                 183.0  0.99215  3.29       0.46   \n",
       "3009                 50.0                 221.0  0.99941  2.94       0.64   \n",
       "4493                 30.0                 165.0  0.99720  3.36       0.55   \n",
       "\n",
       "      alcohol  quality  \n",
       "411      9.60        6  \n",
       "3263    12.60        8  \n",
       "3753    10.70        5  \n",
       "3009     8.80        6  \n",
       "4493    10.55        6  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcnZPS2lQmp8"
   },
   "source": [
    "## Evaluate Logistic Regression using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "TYYUMd9vesi7",
    "outputId": "d91db393-8d2d-4108-e1c3-cf344d3c1553"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score without scaler: 0.990148\n",
      "test score with scaler: 0.993846\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "logistic_cv = LogisticRegression(penalty=\"none\",max_iter=5000)\n",
    "logistic_cv.fit(X_train,y_train)\n",
    "\n",
    "print(\"CV score without scaler: {:.6f}\".format(np.mean(cross_val_score(\n",
    "    logistic_cv, X_train, y_train, cv=5))))\n",
    "print(\"test score with scaler: {:.6f}\".format(logistic_cv.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "031gAf3tf5o3"
   },
   "source": [
    "## Train with Logistic Regression using Cross-Validation with standard scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "tbncdU8Qe82Y",
    "outputId": "0cee5642-7592-43b9-bc47-ea3a7b8b3350"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score with scaler: 0.994869\n",
      "test score with sclaer: 0.993846\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "logistic_cv = LogisticRegression(penalty=\"none\",max_iter=5000)\n",
    "logistic_cv.fit(X_train_scaled,y_train)\n",
    "\n",
    "print(\"CV score with scaler: {:.6f}\".format(np.mean(cross_val_score(\n",
    "    logistic_cv, X_train_scaled, y_train, cv=5))))\n",
    "print(\"test score with sclaer: {:.6f}\".format(logistic_cv.score(X_test_scaled, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdWVJCi7jZPU"
   },
   "outputs": [],
   "source": [
    "From Logistic Regression, we can see that the standard scaler did not improve the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bs0ZxxOedfm_"
   },
   "source": [
    "## Evaluate Penalized Logistic Regression using cross-validation. penalty='l2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "r-BQk0Ungn1F",
    "outputId": "2d18d240-cbf2-4c15-b115-5715fd97b4a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for GrideSeachCV with LogisticRegressionCV is:  0.984301415289868\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create standardizer\n",
    "standardizer = StandardScaler()\n",
    "# Create logistic regression\n",
    "logit = LogisticRegression(penalty=\"l2\",max_iter=5000,solver=\"lbfgs\")\n",
    "\n",
    "# Create a pipeline that standardizes, then runs logistic regression\n",
    "pipeline = make_pipeline(logit)\n",
    "\n",
    "# Create k-Fold cross-validation\n",
    "# kf = KFold()\n",
    "\n",
    "# Do k-fold cross-validation\n",
    "cv_results = cross_val_score(pipeline, # Pipeline\n",
    "                             X, # Feature matrix\n",
    "                             y, # Target vector\n",
    "                             scoring=\"accuracy\", # Loss function\n",
    "                             n_jobs=-1) # Use all CPU scores\n",
    "\n",
    "# Calculate mean\n",
    "cv_results.mean()\n",
    "\n",
    "\n",
    "print('The score for GrideSeachCV with LogisticRegressionCV is: ',cv_results.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "Pd-NvYqDgCNd",
    "outputId": "32882b8f-6ad1-400c-c471-52e505ac9145"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score without scaler: 0.985428\n",
      "test without scaler: 0.980923\n"
     ]
    }
   ],
   "source": [
    "logistic_cvl2 = LogisticRegression(penalty=\"l2\",max_iter=5000,solver=\"lbfgs\")\n",
    "logistic_cvl2.fit(X_train,y_train)\n",
    "\n",
    "print(\"CV score without scaler: {:.6f}\".format(np.mean(cross_val_score(\n",
    "    logistic_cvl2, X_train, y_train, cv=5))))\n",
    "print(\"test without scaler: {:.6f}\".format(logistic_cvl2.score(X_test, y_test)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vMp4If-gx-O"
   },
   "source": [
    "## Evaluate Penalized Logistic Regression using cross-validation with standard scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "4lpMF-QcgXg4",
    "outputId": "d77f3ba4-b1a2-4c7c-f622-79bc7fa7dbfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score with scaler: 0.9945\n",
      "test score with scaler: 0.9945\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "logistic_cvl2 = LogisticRegression(penalty=\"l2\",max_iter=5000,solver=\"lbfgs\")\n",
    "logistic_cvl2.fit(X_train_scaled,y_train)\n",
    "\n",
    "print(\"CV score with scaler: {:.4f}\".format(np.mean(cross_val_score(\n",
    "    logistic_cvl2, X_train_scaled, y_train, cv=5))))\n",
    "print(\"test score with scaler: {:.4f}\".format(logistic_cvl2.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIiMwMqokDKx"
   },
   "source": [
    "From Logistic Regression with penalty l2, we can see that the standard scaler did improve the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g__aHdiokaD5"
   },
   "source": [
    "## Evaluate Penalized Logistic Regression using cross-validation. penalty='l1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "V79R9Pxuknmq",
    "outputId": "a01498f2-3eae-4084-cac5-8af23cb1d40a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for GrideSeachCV with LogisticRegressionCV is:  0.9869175105110439\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create standardizer\n",
    "standardizer = StandardScaler()\n",
    "# Create logistic regression\n",
    "logit = LogisticRegression(penalty=\"l1\",max_iter=5000,solver=\"liblinear\")\n",
    "\n",
    "# Create a pipeline that standardizes, then runs logistic regression\n",
    "pipeline = make_pipeline(logit)\n",
    "\n",
    "# Create k-Fold cross-validation\n",
    "# kf = KFold()\n",
    "\n",
    "# Do k-fold cross-validation\n",
    "cv_results = cross_val_score(pipeline, # Pipeline\n",
    "                             X, # Feature matrix\n",
    "                             y, # Target vector\n",
    "                             scoring=\"accuracy\", # Loss function\n",
    "                             n_jobs=-1) # Use all CPU scores\n",
    "\n",
    "# Calculate mean\n",
    "cv_results.mean()\n",
    "\n",
    "\n",
    "print('The score for GrideSeachCV with LogisticRegressionCV is: ',cv_results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "5t03AEl9k5jd",
    "outputId": "f05dcbf2-ac92-49cc-9904-424fe4417e9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score without scaler: 0.9854\n",
      "test score without sclaer: 0.7668\n"
     ]
    }
   ],
   "source": [
    "logistic_cvl1 = LogisticRegression(penalty=\"l1\",max_iter=5000,solver=\"liblinear\")\n",
    "logistic_cvl1.fit(X_train,y_train)\n",
    "\n",
    "print(\"CV score without scaler: {:.4f}\".format(np.mean(cross_val_score(\n",
    "    logistic_cvl2, X_train, y_train, cv=5))))\n",
    "print(\"test score without sclaer: {:.4f}\".format(logistic_cvl2.score(X_test, y_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbr07BsMlRA8"
   },
   "source": [
    "## Evaluate Penalized l1 Logistic Regression using cross-validation with standard scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "c53byHJ8lTZN",
    "outputId": "b27736c5-12a3-4a15-ae0c-f9556fcbd203"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score with scaler: 0.9947\n",
      "test score with scaler: 0.9938\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "logistic_cvl1 = LogisticRegression(penalty=\"l1\",max_iter=5000,solver=\"liblinear\")\n",
    "logistic_cvl1.fit(X_train_scaled,y_train)\n",
    "\n",
    "print(\"CV score with scaler: {:.4f}\".format(np.mean(cross_val_score(\n",
    "    logistic_cvl1, X_train_scaled, y_train, cv=5))))\n",
    "print(\"test score with scaler: {:.4f}\".format(logistic_cvl1.score(X_test_scaled, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result, we can clearly see that standard scaler help with the score for Penalized l1 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQ6qx8yj0Nal"
   },
   "source": [
    "## Evaluate KNN for classification using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAF4yDrmdGlZ"
   },
   "source": [
    "Instead of arbitrarily choosing alpha , it would be better to use cross-validation to choose the tuning parameter alpha. We can do this using the cross-validated ridge regression function, RidgeCV(). By default, the function performs generalized cross-validation (an efficient form of LOOCV), though this can be changed using the argument cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "dfOEfnMU0u7X",
    "outputId": "181b1934-7d8b-4a31-8360-138de6479281"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for KNN with CV is:  0.9379734707171197\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "\n",
    "# Create a KNN Classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Create a pipeline that standardizes, then runs logistic regression\n",
    "pipeline = make_pipeline(knn)\n",
    "\n",
    "# Create k-Fold cross-validation\n",
    "# kf = KFold()\n",
    "\n",
    "# Do k-fold cross-validation\n",
    "cv_results = cross_val_score(pipeline, # Pipeline\n",
    "                             X, # Feature matrix\n",
    "                             y, # Target vector\n",
    "                             scoring=\"accuracy\", # Loss function\n",
    "                             n_jobs=-1) # Use all CPU scores\n",
    "\n",
    "# Calculate mean\n",
    "cv_results.mean()\n",
    "\n",
    "\n",
    "print('The score for KNN with CV is: ',cv_results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f99z2hsxear3"
   },
   "source": [
    "## Evaluate KNN for classification using with standard  cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "zGEw1YTeNt1j",
    "outputId": "3dc29a0d-5bc3-419a-9725-f6a196a00566"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for KNN with standarlized CV is:  0.9913807070527625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier().fit(X, y)\n",
    "\n",
    "classifier_pipeline = make_pipeline(preprocessing.StandardScaler(), knn)\n",
    "\n",
    "# KFold/StratifiedKFold cross validation with 3 folds (the default)\n",
    "# applying the classifier pipeline to the feature and target data\n",
    "scores = cross_val_score(classifier_pipeline, X, y)\n",
    "\n",
    "scores\n",
    "\n",
    "scores.mean()\n",
    "\n",
    "print('The score for KNN with standarlized CV is: ',scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can claearly see that scaler help KNN in its scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YRceZHqk3UN"
   },
   "source": [
    "## 2.3 Tune the parameters where possible using GridSearchCV. Do the results improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmoebzQYoa8c"
   },
   "source": [
    "The logistic regression does not has a tuning parameter, so I just ignore it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrw4Z6-Alcj1"
   },
   "source": [
    "## GridSearchCV with l1 Penalized Losistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "iixJM55mmSzC",
    "outputId": "a5046842-ae27-4b4e-a3bb-423f960754eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mean cross-validation score: 0.994664\n",
      "best parameters: {'C': 1}\n",
      "test-set score: 0.993846\n"
     ]
    }
   ],
   "source": [
    "params = {\"C\": [1,0.9999999,0.9999,0.9,0.5,0.1,0.025,0.01,0.0025,0.001]}\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(max_iter=1000,penalty=\"l1\",solver=\"liblinear\"),param_grid = params,cv=5)\n",
    "\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Mean cross-validated score of the best_estimator\n",
    "print(\"best mean cross-validation score: {:.6f}\".format(grid.best_score_))\n",
    "# Parameter setting that gave the best results on the hold out data.\n",
    "print(\"best parameters: {}\".format(grid.best_params_))\n",
    "print(\"test-set score: {:.6f}\".format(grid.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lyr7eMRvnJSw"
   },
   "source": [
    "## GridSearchCV with l2 Penalized Losistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "j-KqB5eRmhme",
    "outputId": "2c649fbd-b8bc-4f68-b2ec-493a249fed2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mean cross-validation score: 0.994459\n",
      "best parameters: {'C': 1}\n",
      "test-set score: 0.994462\n"
     ]
    }
   ],
   "source": [
    "params = {\"C\": [1,0.9999999,0.9999,0.9,0.5,0.1,0.025,0.01,0.0025,0.001]}\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(max_iter=1000,penalty=\"l2\",solver=\"lbfgs\"),param_grid = params,cv=5)\n",
    "\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Mean cross-validated score of the best_estimator\n",
    "print(\"best mean cross-validation score: {:.6f}\".format(grid.best_score_))\n",
    "# Parameter setting that gave the best results on the hold out data.\n",
    "print(\"best parameters: {}\".format(grid.best_params_))\n",
    "print(\"test-set score: {:.6f}\".format(grid.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAwaXlx-plXL"
   },
   "source": [
    "## GridSearchCV with KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "IpP0MabHnvPR",
    "outputId": "87b4f606-2da3-4f76-c6fd-34975190374e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mean cross-validation score: 0.9945\n",
      "best parameters: {'n_neighbors': 4}\n",
      "test-set score: 0.9932\n"
     ]
    }
   ],
   "source": [
    "# KNN Regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#grid\n",
    "params = {\"n_neighbors\": np.arange(1,20,3)}\n",
    "\n",
    "grid = GridSearchCV(KNeighborsClassifier(),param_grid = params, cv=5)\n",
    "\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Mean cross-validated score of the best_estimator\n",
    "print(\"best mean cross-validation score: {:.4f}\".format(grid.best_score_))\n",
    "# Parameter setting that gave the best results on the hold out data.\n",
    "print(\"best parameters: {}\".format(grid.best_params_))\n",
    "print(\"test-set score: {:.4f}\".format(grid.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Maq3Q2Sgcoqb"
   },
   "source": [
    "## 2.4. Change the cross-validation strategy in GridSearchCV from stratified k-fold to kfold with shuffling. Do the parameters for models that can be tuned change? Do they change if you change the random seed of the shuffling? Or if you change the random state of the split into training and test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmPTUUl6qmWf"
   },
   "source": [
    "## GridSearchCV with Logistic Regression penalty l1 stratified K-FLOD and KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "tK7UoVhVqrWM",
    "outputId": "faffe920-2c67-41df-b503-601a958ce8ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg-l1 best mean cross-validation score: 0.9947\n",
      "LogReg-l1 best parameters: {'C': 1}\n",
      "LogReg-l1 test-set score: 0.9938\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression penalty l1 - StratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# Cross validate model with Kfold stratified grid cross valb\n",
    "skfold = StratifiedKFold(n_splits=10,random_state=0,shuffle=True)\n",
    "\n",
    "# Logistic Regression with Penalty l1\n",
    "params = {\"C\": [1,0.9999999,0.9999,0.9,0.5,0.1,0.025,0.01,0.0025,0.001]}\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(max_iter=5000,penalty='l1',solver=\"liblinear\"),param_grid = params,cv=skfold)\n",
    "\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Mean cross-validated score of the best_estimator\n",
    "print(\"LogReg-l1 best mean cross-validation score: {:.4f}\".format(grid.best_score_))\n",
    "# Parameter setting that gave the best results on the hold out data.\n",
    "print(\"LogReg-l1 best parameters: {}\".format(grid.best_params_))\n",
    "print(\"LogReg-l1 test-set score: {:.4f}\".format(grid.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "3LGWI2rLuQ6K",
    "outputId": "d85ae033-93ad-4cf0-b1d6-47bebc6defe4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg-l1 best mean cross-validation score: 0.9953\n",
      "LogReg-l1 best parameters: {'C': 1}\n",
      "LogReg-l1 test-set score: 0.9938\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression penalty l1 - KFold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "# Cross validate model with Kfold grid cross valb\n",
    "kfold = KFold(n_splits=10,random_state=0,shuffle=True)\n",
    "\n",
    "# Logistic Regression with Penalty l1\n",
    "params = {\"C\": [1,0.9999999,0.9999,0.9,0.5,0.1,0.025,0.01,0.0025,0.001]}\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(max_iter=5000,penalty='l1',solver=\"liblinear\"),param_grid = params,cv=kfold)\n",
    "\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Mean cross-validated score of the best_estimator\n",
    "print(\"LogReg-l1 best mean cross-validation score: {:.4f}\".format(grid.best_score_))\n",
    "# Parameter setting that gave the best results on the hold out data.\n",
    "print(\"LogReg-l1 best parameters: {}\".format(grid.best_params_))\n",
    "print(\"LogReg-l1 test-set score: {:.4f}\".format(grid.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "mwvjfmeuxOwa",
    "outputId": "db6165ad-01c9-49e6-c007-624a36fcb257"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "when random_state =  0\n",
      "LogReg-l1 best parameters: {'C': 1}\n",
      " \n",
      "when random_state =  1\n",
      "LogReg-l1 best parameters: {'C': 0.1}\n",
      " \n",
      "when random_state =  50\n",
      "LogReg-l1 best parameters: {'C': 1}\n",
      " \n",
      "when random_state =  100\n",
      "LogReg-l1 best parameters: {'C': 0.1}\n",
      " \n",
      "when random_state =  100\n",
      "LogReg-l1 best parameters: {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression penalty l1 - Change random seed of shuffling\n",
    "\n",
    "\n",
    "\n",
    "for i in [0,1,50,100,100]:\n",
    "    kfold = KFold(n_splits=10,random_state=i,shuffle=True)\n",
    "    print(\" \")\n",
    "    print(\"when random_state = \", i)\n",
    "\n",
    "    params = {\"C\": [1,0.9999999,0.9999,0.5,0.1,0.025,0.01,0.001]}\n",
    "\n",
    "    grid = GridSearchCV(LogisticRegression(max_iter=1000,penalty='l1',solver=\"liblinear\"),param_grid = params,cv=kfold)\n",
    "\n",
    "    grid.fit(X_train_scaled, y_train)\n",
    "    \n",
    "\n",
    "   # Parameter setting that gave the best results on the hold out data.\n",
    "    print(\"LogReg-l1 best parameters: {}\".format(grid.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwM9mHJAy3ea"
   },
   "source": [
    "It changes the best parameter C for Losistic regreesion if we change the random state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1idvjakuGfp"
   },
   "source": [
    "## GridSearchCV with Logistic Regression penalty l2 stratified K-FLOD and KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "SjdYsdqjuvsG",
    "outputId": "e2fc5a93-fbd2-4b24-ff7c-08bf280b4e0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg-l2 best mean cross-validation score: 0.9945\n",
      "LogReg-l2 best parameters: {'C': 1}\n",
      "LogReg-l2 test-set score: 0.9945\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression penalty l2 - StratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# Cross validate model with Kfold stratified grid cross valb\n",
    "skfold = StratifiedKFold(n_splits=10,random_state=0,shuffle=True)\n",
    "\n",
    "# Logistic Regression with Penalty l2\n",
    "\n",
    "params = {\"C\": [1,0.9999999,0.9999,0.9,0.5,0.1,0.025,0.01,0.0025,0.001]}\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(max_iter=5000,penalty='l2',solver=\"lbfgs\"),param_grid = params,cv=skfold)\n",
    "\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Mean cross-validated score of the best_estimator\n",
    "print(\"LogReg-l2 best mean cross-validation score: {:.4f}\".format(grid.best_score_))\n",
    "# Parameter setting that gave the best results on the hold out data.\n",
    "print(\"LogReg-l2 best parameters: {}\".format(grid.best_params_))\n",
    "print(\"LogReg-l2 test-set score: {:.4f}\".format(grid.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "hBC61Zuru2FX",
    "outputId": "7b99fc51-2f33-4217-abf4-884fda70575c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg-l2 best mean cross-validation score: 0.9940\n",
      "LogReg-l2 best parameters: {'C': 0.5}\n",
      "LogReg-l2 test-set score: 0.9938\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression penalty l2 - KFold\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "# Cross validate model with Kfold grid cross valb\n",
    "kfold = KFold(n_splits=10,random_state=0,shuffle=True)\n",
    "\n",
    "# Logistic Regression with Penalty l1\n",
    "params = {\"C\": [1,0.9999999,0.9999,0.9,0.5,0.1,0.025,0.01,0.0025,0.001]}\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(max_iter=5000,penalty='l2',solver=\"lbfgs\"),param_grid = params,cv=kfold)\n",
    "\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Mean cross-validated score of the best_estimator\n",
    "print(\"LogReg-l2 best mean cross-validation score: {:.4f}\".format(grid.best_score_))\n",
    "# Parameter setting that gave the best results on the hold out data.\n",
    "print(\"LogReg-l2 best parameters: {}\".format(grid.best_params_))\n",
    "print(\"LogReg-l2 test-set score: {:.4f}\".format(grid.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "hwgKy2krzK5s",
    "outputId": "ba414b95-c2f7-43e6-c68f-94655d53c96a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "when random_state =  0\n",
      "LogReg-l1 best parameters: {'C': 0.5}\n",
      " \n",
      "when random_state =  1\n",
      "LogReg-l1 best parameters: {'C': 1}\n",
      " \n",
      "when random_state =  50\n",
      "LogReg-l1 best parameters: {'C': 1}\n",
      " \n",
      "when random_state =  100\n",
      "LogReg-l1 best parameters: {'C': 1}\n",
      " \n",
      "when random_state =  100\n",
      "LogReg-l1 best parameters: {'C': 1}\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression penalty l2 - Change of randomw state\n",
    "\n",
    "\n",
    "for i in [0,1,50,100,100]:\n",
    "    kfold = KFold(n_splits=10,random_state=i,shuffle=True)\n",
    "    print(\" \")\n",
    "    print(\"when random_state = \", i)\n",
    "\n",
    "    params = {\"C\": [1,0.9999999,0.9999,0.5,0.1,0.025,0.01,0.001]}\n",
    "\n",
    "    grid = GridSearchCV(LogisticRegression(max_iter=1000,penalty='l2'),param_grid = params,cv=kfold)\n",
    "\n",
    "    grid.fit(X_train_scaled, y_train)\n",
    "    \n",
    "\n",
    "   # Parameter setting that gave the best results on the hold out data.\n",
    "    print(\"LogReg-l1 best parameters: {}\".format(grid.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnBPmoO1zmEI"
   },
   "source": [
    "From this we can see that change of the random state does lead to a change in the best parameters, since the C goes form 0.5 to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MnAhGcAqxZV"
   },
   "source": [
    "## GridSearchCV with KNN stratified K-FLOD and KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "UmFDD7J3vVwt",
    "outputId": "475b179c-68b1-4bfb-85d5-23c65d95f612"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN best mean cross-validation score: 0.994047\n",
      "KNN best parameters: {'n_neighbors': 3}\n",
      "KNN test-set score: 0.991385\n"
     ]
    }
   ],
   "source": [
    "# KNN Regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# try more n_neighbors\n",
    "params = {\"n_neighbors\": np.arange(1,17,2)}\n",
    "\n",
    "grid = GridSearchCV(KNeighborsClassifier(),param_grid = params, cv=skfold)\n",
    "\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Mean cross-validated score of the best_estimator\n",
    "print(\"KNN best mean cross-validation score: {:.6f}\".format(grid.best_score_))\n",
    "# Parameter setting that gave the best results on the hold out data.\n",
    "print(\"KNN best parameters: {}\".format(grid.best_params_))\n",
    "print(\"KNN test-set score: {:.6f}\".format(grid.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "cW-lNB28vb6m",
    "outputId": "bf45cdae-e530-4bf0-b276-8d16f2a47b98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN best mean cross-validation score: 0.993842\n",
      "KNN best parameters: {'n_neighbors': 3}\n",
      "KNN test-set score: 0.991385\n"
     ]
    }
   ],
   "source": [
    "# KNN Regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# try more n_neighbors\n",
    "params = {\"n_neighbors\": np.arange(1,17,2)}\n",
    "\n",
    "grid = GridSearchCV(KNeighborsClassifier(),param_grid = params, cv=kfold)\n",
    "\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Mean cross-validated score of the best_estimator\n",
    "print(\"KNN best mean cross-validation score: {:.6f}\".format(grid.best_score_))\n",
    "# Parameter setting that gave the best results on the hold out data.\n",
    "print(\"KNN best parameters: {}\".format(grid.best_params_))\n",
    "print(\"KNN test-set score: {:.6f}\".format(grid.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "qdKhc53R0FNN",
    "outputId": "e2aa34e2-ed48-4d31-cc37-1c7c6e3d9e8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "when random_state =  0\n",
      "KNN best parameters: {'n_neighbors': 3}\n",
      "\n",
      "when random_state =  1\n",
      "KNN best parameters: {'n_neighbors': 5}\n",
      "\n",
      "when random_state =  50\n",
      "KNN best parameters: {'n_neighbors': 1}\n",
      "\n",
      "when random_state =  100\n",
      "KNN best parameters: {'n_neighbors': 3}\n",
      "\n",
      "when random_state =  100\n",
      "KNN best parameters: {'n_neighbors': 3}\n"
     ]
    }
   ],
   "source": [
    "# KNN change of random state\n",
    "\n",
    "\n",
    "for i in [0,1,50,100,100]:\n",
    "    kfold = KFold(n_splits=10,random_state=i,shuffle=True)\n",
    "    print(\"\")\n",
    "    print(\"when random_state = \", i)\n",
    "    # KNN Regression\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    # try more n_neighbors\n",
    "    params = {\"n_neighbors\": np.arange(1,17,2)}\n",
    "\n",
    "    grid = GridSearchCV(KNeighborsClassifier(),param_grid = params, cv=kfold)\n",
    "\n",
    "    grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "    print(\"KNN best parameters: {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YuYsGfW0cDy"
   },
   "source": [
    "From this see can see that change of random state does influence KNN's n_neighbors, from Random state = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4K-iEglhx8l"
   },
   "source": [
    "## 2.5 Lastly, compare the coefficients for Logistic Regression and Penalized Logistic Regression and discuss which final model you would choose to predict new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "Yw3GJijE0uBS",
    "outputId": "75557d43-335b-475f-c54c-a1d9f94ee5db",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coef_logReg:  [[-0.25664567  0.95115316 -0.39634261 -4.34811316  0.81549868  0.97958568\n",
      "  -2.73609813  4.99256598 -0.18971012  0.47088311  1.95771063  0.35784019]]\n"
     ]
    }
   ],
   "source": [
    "coef_logReg = logistic_cv.coef_\n",
    "print(\"coef_logReg: \",coef_logReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coef_logReg_l1:  [[ 0.          0.97859396 -0.35744029 -3.67642051  0.80233971  0.80461357\n",
      "  -2.59702371  4.25665922  0.          0.51693888  1.59113037  0.32151015]]\n"
     ]
    }
   ],
   "source": [
    "# c = 0.9 for l1\n",
    "coef_logReg_l1 = LogisticRegression(penalty=\"l1\",max_iter=5000,solver=\"liblinear\",C=0.9).fit(X_train_scaled,y_train).coef_\n",
    "print(\"coef_logReg_l1: \",coef_logReg_l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coef_logReg_l2:  [[ 0.32092103  1.07271369 -0.34557511 -3.07403774  0.85585695  0.70628661\n",
      "  -2.57382963  3.42902538  0.24240518  0.64173163  1.20764483  0.29859525]]\n"
     ]
    }
   ],
   "source": [
    "# c = 1 for l2\n",
    "coef_logReg_l2 = LogisticRegression(penalty=\"l2\",max_iter=5000,solver=\"lbfgs\",C=1).fit(X_train_scaled,y_train).coef_\n",
    "print(\"coef_logReg_l2: \",coef_logReg_l2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1GFLT-31gLx"
   },
   "source": [
    "From the coefficient we can see that the three models all agree that the 8th featyre, density, is the most important feature.\n",
    "\n",
    "If I could, I would like to choose the Logistic Regression with Penalty l1 because it gets rid of some un-important features and focus more on the important features"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "qjmXrV_MMF8x",
    "GcnZPS2lQmp8",
    "031gAf3tf5o3",
    "Bs0ZxxOedfm_",
    "_vMp4If-gx-O",
    "g__aHdiokaD5",
    "qbr07BsMlRA8",
    "ZQ6qx8yj0Nal",
    "f99z2hsxear3",
    "zrw4Z6-Alcj1",
    "Lyr7eMRvnJSw",
    "qAwaXlx-plXL",
    "mmPTUUl6qmWf",
    "z1idvjakuGfp",
    "8MnAhGcAqxZV"
   ],
   "name": "Q2. ML HW2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
